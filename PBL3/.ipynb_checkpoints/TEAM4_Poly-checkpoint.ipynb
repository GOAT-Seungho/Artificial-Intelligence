{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQpD1XmZ6f6q"
   },
   "source": [
    "# Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "VfRhw-6M3R4x",
    "outputId": "e4c6e9ad-0c70-4beb-c59c-ef7fded602ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lf2kcWr2xfS5"
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JX3uv-d9whWt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import struct\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyadthozxkvE"
   },
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUQO7TzZxVH4"
   },
   "outputs": [],
   "source": [
    "class BinaryClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, batch_size=16, max_iter=100, learning_rate=0.01, random_state=1, C=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Exception Handling\n",
    "        if self.C < 0:\n",
    "            raise ValueError(\"The C value of %r must be positive\" % self.C)\n",
    "        if ((self.learning_rate < 0) or (self.learning_rate > 1)):\n",
    "            raise ValueError(\"The learning_rate value of %r is invalid.\" % self.learning_rate,\n",
    "                             \"Set the learning_rate value between 0.0 and 1.0.\")        \n",
    "            \n",
    "        n_batches = math.ceil(len(X) / self.batch_size)\n",
    "        # Process the total number of data is not divided into batch size\n",
    "        rest_batch_size = X.shape[0] - (n_batches-1) * self.batch_size\n",
    "        \n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = 0.\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            X, y = self.shuffle(X, y)\n",
    "            \n",
    "            Parallel(n_jobs=-1, require=\"sharedmem\")(\n",
    "                delayed(self.calculateGradientAndUpdate)(X, y, batch_size = self.batch_size, n_batch = j)\n",
    "                for j in range(n_batches - 1)\n",
    "            )\n",
    "            self.calculateGradientAndUpdate(X, y, batch_size = rest_batch_size, n_batch = j)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.hypothesis(X) >= 1, 1, -1)\n",
    "    \n",
    "    def hypothesis(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def shuffle(self, X, y):\n",
    "        shuffle_index = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffle_index)\n",
    "        return X[shuffle_index], y[shuffle_index]\n",
    "    \n",
    "    def calculateGradientAndUpdate(self, X, y, batch_size, n_batch):\n",
    "        X_mini = X[n_batch*batch_size : (n_batch+1)*batch_size]\n",
    "        y_mini = y[n_batch*batch_size : (n_batch+1)*batch_size]\n",
    "        \n",
    "        grad_w = np.zeros(X.shape[1])\n",
    "        grad_b = 0\n",
    "        mask = np.less_equal(np.multiply(y_mini, self.hypothesis(X_mini), 1)\n",
    "        \n",
    "        Xy = np.multiply(X_mini.T, y_mini)\n",
    "        masked_Xy = np.multiply(Xy, mask)\n",
    "        grad_w = (np.sum(-masked_Xy, axis=1) / batch_size) + self.w_/self.C\n",
    "        self.w_ -= self.learning_rate * grad_w\n",
    "        \n",
    "        masked_y = np.multiply(y_mini, mask)\n",
    "        grad_b = np.sum(-masked_y, axis=0) / batch_size\n",
    "        self.b_ -= self.learning_rate * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yQihMyWxso8"
   },
   "source": [
    "# Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8jHzGocxwLm"
   },
   "outputs": [],
   "source": [
    "class MulticlassClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, batch_size=16, max_iter=100, learning_rate=0.01, random_state=1, C=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.labels = np.unique(y) # 0 ~ 9\n",
    "        self.outputs_ = []\n",
    "        for label in range(len(self.labels)):\n",
    "            y_binary = np.where(y == label, 1, -1)\n",
    "            b_c = BinaryClassifier(self.batch_size, self.max_iter, \n",
    "                                   self.learning_rate, self.random_state, self.C)\n",
    "            b_c.fit(X, y_binary)\n",
    "            self.outputs_.append(b_c)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        prediction = []\n",
    "        for o in self.outputs_:\n",
    "            prediction.append(o.hypothesis(X))\n",
    "        return self.labels[np.argmax(prediction, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4zskV6XyTNX"
   },
   "source": [
    "# MNIST Read Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_cA1d2FxwhE"
   },
   "outputs": [],
   "source": [
    "def read(images, labels):\n",
    "    with open(labels, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def read_no_label(images):\n",
    "    with open(images, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(60000, 784)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NM_gcg_X6u4u"
   },
   "source": [
    "# Read MNIST & Split for Valiation (80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viA-V_NGySjf"
   },
   "outputs": [],
   "source": [
    "                            # 경로 수정하세요 !\n",
    "X, y = read(os.getcwd() + '/data/newtrain-images-idx3-ubyte', \n",
    "            os.getcwd() + '/data/newtrain-labels-idx1-ubyte')\n",
    "# X_test_no_label = read_no_label(os.getcwd()+'/data/testall-images-idx3-ubyte')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVHgeOni7HQn"
   },
   "source": [
    "# Preprocessing : StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HB6tbaNm7T5A"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SakUhQM57WH7"
   },
   "source": [
    "# Preprocessing : PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNXZi3G1Hap8"
   },
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_scaled.reshape(-1, 28*28)\n",
    "pca = PCA(n_components=50)\n",
    "X_train_scaled_pca = pca.fit_transform(X_train_scaled) \n",
    "X_test_scaled_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MV4HbBAZ7eUr"
   },
   "source": [
    "# Preprocessing : Polynomial Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aogZ4O0I0S12"
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False, order='F')\n",
    "X_train_scaled_pca_poly = poly.fit_transform(X_train_scaled_pca)\n",
    "X_test_scaled_pca_poly = poly.transform(X_test_scaled_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6WaCb0g7uV_"
   },
   "source": [
    "## Check how many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ULHS8sbD-5xY",
    "outputId": "21548297-59ad-4e20-dc84-bd3fff3ccf79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(56000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train_scaled_pca.shape)\n",
    "print(X_train_scaled_pca_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mevK6cEc8EoL"
   },
   "source": [
    "# Set the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llpouwU62bvk"
   },
   "outputs": [],
   "source": [
    "MC=MulticlassClassifier(C=1000, learning_rate=0.01, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yXxo4nl8N5C"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xSCTXcp8jV5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time :  2019-12-02 22:09:47.053450\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Time : \", datetime.now())\n",
    "\n",
    "start = time.time()\n",
    "MC.fit(X_train_scaled_pca_poly, y_train)\n",
    "\n",
    "print(\"End Time : \", datetime.now())\n",
    "print(\"Training Time : \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeKh4PZF8kl8"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "7bR90WGMvd8l",
    "outputId": "0b74bbfc-0978-4a2c-bb3c-50d92cd4d350"
   },
   "outputs": [],
   "source": [
    "y_pred = MC.predict(X_test_scaled_pca_poly)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TEAM4-Poly.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
