{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQpD1XmZ6f6q"
   },
   "source": [
    "# Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "VfRhw-6M3R4x",
    "outputId": "78a8b6a9-a3e7-4f43-fe72-0f5fd2acced5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lf2kcWr2xfS5"
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JX3uv-d9whWt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import struct\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyadthozxkvE"
   },
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUQO7TzZxVH4"
   },
   "outputs": [],
   "source": [
    "class BinaryClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, batch_size=16, max_iter=100, learning_rate=0.01, random_state=1, C=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Exception Handling\n",
    "        if self.C < 0:\n",
    "            raise ValueError(\"The C value of %r must be positive\" % self.C)\n",
    "        if ((self.learning_rate < 0) or (self.learning_rate > 1)):\n",
    "            raise ValueError(\"The learning_rate value of %r is invalid.\" % self.learning_rate,\n",
    "                             \"Set the learning_rate value between 0.0 and 1.0.\")        \n",
    "            \n",
    "        n_batches = math.ceil(len(X) / self.batch_size)\n",
    "        # Process the total number of data is not divided into batch size\n",
    "        rest_batch_size = X.shape[0] - (n_batches-1) * self.batch_size\n",
    "        \n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = 0.\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            X, y = self.shuffle(X, y)\n",
    "            \n",
    "            Parallel(n_jobs=-1, require=\"sharedmem\")(\n",
    "                delayed(self.calculateGradientAndUpdate)(X, y, batch_size = self.batch_size, n_batch = j)\n",
    "                for j in range(n_batches - 1)\n",
    "            )\n",
    "            self.calculateGradientAndUpdate(X, y, batch_size = rest_batch_size, n_batch = j)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.hypothesis(X) >= 1, 1, -1)\n",
    "    \n",
    "    def hypothesis(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def shuffle(self, X, y):\n",
    "        shuffle_index = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffle_index)\n",
    "        return X[shuffle_index], y[shuffle_index]\n",
    "    \n",
    "    def calculateGradientAndUpdate(self, X, y, batch_size, n_batch):\n",
    "        X_mini = X[n_batch*batch_size : (n_batch+1)*batch_size]\n",
    "        y_mini = y[n_batch*batch_size : (n_batch+1)*batch_size]\n",
    "        \n",
    "        grad_w = np.zeros(X.shape[1])\n",
    "        grad_b = 0\n",
    "        mask = np.less_equal(np.multiply(y_mini, self.hypothesis(X_mini)), 1)\n",
    "        \n",
    "        Xy = np.multiply(X_mini.T, y_mini)\n",
    "        masked_Xy = np.multiply(Xy, mask)\n",
    "        grad_w = (np.sum(-masked_Xy, axis=1) / batch_size) + self.w_/self.C\n",
    "        self.w_ -= self.learning_rate * grad_w\n",
    "        \n",
    "        masked_y = np.multiply(y_mini, mask)\n",
    "        grad_b = np.sum(-masked_y, axis=0) / batch_size\n",
    "        self.b_ -= self.learning_rate * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yQihMyWxso8"
   },
   "source": [
    "# Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8jHzGocxwLm"
   },
   "outputs": [],
   "source": [
    "class MulticlassClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, batch_size=16, max_iter=100, learning_rate=0.01, random_state=1, C=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.labels = np.unique(y) # 0 ~ 9\n",
    "        self.outputs_ = []\n",
    "        for label in range(len(self.labels)):\n",
    "            y_binary = np.where(y == label, 1, -1)\n",
    "            b_c = BinaryClassifier(self.batch_size, self.max_iter, \n",
    "                                   self.learning_rate, self.random_state, self.C)\n",
    "            b_c.fit(X, y_binary)\n",
    "            self.outputs_.append(b_c)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        prediction = []\n",
    "        for o in self.outputs_:\n",
    "            prediction.append(o.hypothesis(X))\n",
    "        return self.labels[np.argmax(prediction, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4zskV6XyTNX"
   },
   "source": [
    "# MNIST Read Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_cA1d2FxwhE"
   },
   "outputs": [],
   "source": [
    "def read(images, labels):\n",
    "    with open(labels, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def read_no_label(images):\n",
    "    with open(images, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(60000, 784)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NM_gcg_X6u4u"
   },
   "source": [
    "# Read MNIST & Split for Valiation (80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viA-V_NGySjf"
   },
   "outputs": [],
   "source": [
    "                            # 경로 수정하세요 !\n",
    "X, y = read(os.getcwd() + '/data/newtrain-images-idx3-ubyte', \n",
    "            os.getcwd() + '/data/newtrain-labels-idx1-ubyte')\n",
    "# X_test_no_label = read_no_label(os.getcwd()+'/data/testall-images-idx3-ubyte')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "# X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "# X_test = X_test.reshape(X_test.shape[0], 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYKNh4VglWOt"
   },
   "source": [
    "# Preprocessing : Convolutional Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGTEGyFTmVuk"
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fX4LRWR3lb3i"
   },
   "outputs": [],
   "source": [
    "def zero_padding(img, n=1):\n",
    "\n",
    "    m = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[2]\n",
    "    \n",
    "    padded_img = np.zeros((m, w + 2 * n, h + 2 * n))\n",
    "    \n",
    "    padded_img[:, n : padded_img.shape[1] - n, n : padded_img.shape[2] - n] = img\n",
    "    \n",
    "    return padded_img\n",
    "    \n",
    "def horizontal_filter(img):\n",
    "    h_filter = np.array([\n",
    "        [ 0,  0,  0],\n",
    "        [ 0,  1,  0],\n",
    "        [ 0,  0, -1]\n",
    "    ])\n",
    "    h_filter.reshape(1,3,3)\n",
    "    m = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[2]\n",
    "    horizontal_grad = np.zeros((m, w - 2, h - 2))\n",
    "\n",
    "    for i in range(1, w - 1):\n",
    "        for j in range(1, h - 1):\n",
    "            images_slice = img[:, i - 1 : i + 2, j - 1 : j + 2]\n",
    "            horizontal_grad[:, i - 1, j - 1] = np.sum(np.multiply(images_slice, h_filter), axis=(1, 2))\n",
    "            \n",
    "    return horizontal_grad\n",
    "    \n",
    "def MaxPooling(img):\n",
    "\n",
    "    m = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[2]\n",
    "    \n",
    "    img = zero_padding(img, 1)\n",
    "    img2 = horizontal_filter(img)\n",
    "    \n",
    "    pooling_grad = np.zeros((m, w//2, h//2))\n",
    "    print('pooling_grade.shape[0]', pooling_grad.shape[0])\n",
    "\n",
    "    for i in range(0, w//2):\n",
    "        for j in range(0, h//2):\n",
    "            pooling_grad[: , i , j ] = np.max(img2[: , 2*i : 2*i + 2, 2*j : 2*j + 2])\n",
    "            \n",
    "    return pooling_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5VJlZQkmUUD"
   },
   "source": [
    "## Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "14v7xsPVmcl1",
    "outputId": "c5159640-0695-4de3-8a0a-b844f4243b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-769500201efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-5283c492635a>\u001b[0m in \u001b[0;36mMaxPooling\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzero_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape[0], X_test.shape[0]\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train_pool = MaxPooling(X_train)\n",
    "X_test_pool = MaxPooling(X_test)\n",
    "\n",
    "X_train = X_train.reshape(m, -1)\n",
    "X_test = X_test.reshape(n, -1)\n",
    "X_train_pool = X_train_pool.reshape(m, -1)\n",
    "X_test_pool = X_test_pool.reshape(n, -1)\n",
    "\n",
    "X_train_conv = np.hstack([X_train, X_train_pool])\n",
    "X_test_conv = np.hstack([X_test, X_test_pool])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVHgeOni7HQn"
   },
   "source": [
    "# Preprocessing : StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HB6tbaNm7T5A"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SakUhQM57WH7"
   },
   "source": [
    "# Preprocessing : PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNXZi3G1Hap8"
   },
   "outputs": [],
   "source": [
    "# X_train_scaled = X_train_scaled.reshape(-1, 28*28)\n",
    "pca = PCA(n_components=50)\n",
    "X_train_scaled_pca = pca.fit_transform(X_train_scaled) \n",
    "X_test_scaled_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MV4HbBAZ7eUr"
   },
   "source": [
    "# Preprocessing : Polynomial Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aogZ4O0I0S12"
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False, order='F')\n",
    "X_train_conv_scaled_pca_poly = poly.fit_transform(X_train_conv_scaled_pca)\n",
    "X_test_conv_scaled_pca_poly = poly.transform(X_test_conv_scaled_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6WaCb0g7uV_"
   },
   "source": [
    "## Check how many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULHS8sbD-5xY"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train_conv.shape)\n",
    "print(X_train_conv_scaled.shape)\n",
    "print(X_train_conv_scaled_pca.shape)\n",
    "print(X_train_conv_scaled_pca_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mevK6cEc8EoL"
   },
   "source": [
    "# Set the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llpouwU62bvk"
   },
   "outputs": [],
   "source": [
    "MC=MulticlassClassifier(C=1000, learning_rate=0.01, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yXxo4nl8N5C"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xSCTXcp8jV5"
   },
   "outputs": [],
   "source": [
    "print(\"Start Time : \", datetime.now())\n",
    "\n",
    "start = time.time()\n",
    "MC.fit(X_train_scaled_pca_poly, y_train)\n",
    "\n",
    "print(\"End Time : \", datetime.now())\n",
    "print(\"Training Time : \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeKh4PZF8kl8"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bR90WGMvd8l"
   },
   "outputs": [],
   "source": [
    "y_pred = MC.predict(X_test_scaled_pca_poly)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TEAM4_Poly-Parallel-Conv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
