{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQpD1XmZ6f6q"
   },
   "source": [
    "# Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30081,
     "status": "ok",
     "timestamp": 1576319724888,
     "user": {
      "displayName": "용일조",
      "photoUrl": "",
      "userId": "09844505769840278881"
     },
     "user_tz": -540
    },
    "id": "VfRhw-6M3R4x",
    "outputId": "0d442574-ac28-4ef9-f07c-0310055412f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4x7FW2Rp2T6f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lf2kcWr2xfS5"
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JX3uv-d9whWt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import struct\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyadthozxkvE"
   },
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUQO7TzZxVH4"
   },
   "outputs": [],
   "source": [
    "class BinaryClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, batch_size=16, max_iter=100, learning_rate=0.01, random_state=1, C=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Exception Handling\n",
    "        if self.C < 0:\n",
    "            raise ValueError(\"The C value of %r must be positive\" % self.C)\n",
    "        if ((self.learning_rate < 0) or (self.learning_rate > 1)):\n",
    "            raise ValueError(\"The learning_rate value of %r is invalid.\" % self.learning_rate,\n",
    "                             \"Set the learning_rate value between 0.0 and 1.0.\")        \n",
    "            \n",
    "        n_batches = math.ceil(len(X) / self.batch_size)\n",
    "        # Process the total number of data is not divided into batch size\n",
    "        rest_batch_size = X.shape[0] - (n_batches-1) * self.batch_size\n",
    "        \n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = 0.\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            X, y = self.shuffle(X, y)\n",
    "            \n",
    "            Parallel(n_jobs=-1, require=\"sharedmem\")(\n",
    "                delayed(self.calculateGradientAndUpdate)(X, y, batch_size = self.batch_size, n_batch = j)\n",
    "                for j in range(n_batches - 1)\n",
    "            )\n",
    "            self.calculateGradientAndUpdate(X, y, batch_size = rest_batch_size, n_batch = n_batches-1)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.hypothesis(X) >= 1, 1, -1)\n",
    "    \n",
    "    def hypothesis(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def shuffle(self, X, y):\n",
    "        shuffle_index = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffle_index)\n",
    "        return X[shuffle_index], y[shuffle_index]\n",
    "    \n",
    "    def calculateGradientAndUpdate(self, X, y, batch_size, n_batch):\n",
    "        X_mini = X[n_batch*batch_size : (n_batch+1)*batch_size]\n",
    "        y_mini = y[n_batch*batch_size : (n_batch+1)*batch_size]\n",
    "        \n",
    "        grad_w = np.zeros(X.shape[1])\n",
    "        grad_b = 0\n",
    "        mask = np.less_equal(np.multiply(y_mini, self.hypothesis(X_mini)), 1)\n",
    "        \n",
    "        Xy = np.multiply(X_mini.T, y_mini)\n",
    "        masked_Xy = np.multiply(Xy, mask)\n",
    "        grad_w = (np.sum(-masked_Xy, axis=1) / batch_size) + self.w_/self.C\n",
    "        self.w_ -= self.learning_rate * grad_w\n",
    "        \n",
    "        masked_y = np.multiply(y_mini, mask)\n",
    "        grad_b = np.sum(-masked_y, axis=0) / batch_size\n",
    "        self.b_ -= self.learning_rate * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yQihMyWxso8"
   },
   "source": [
    "# Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8jHzGocxwLm"
   },
   "outputs": [],
   "source": [
    "class MulticlassClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, batch_size=16, max_iter=100, learning_rate=0.01, random_state=1, C=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.labels = np.unique(y) # 0 ~ 9\n",
    "        self.outputs_ = []\n",
    "        for label in range(len(self.labels)):\n",
    "            y_binary = np.where(y == label, 1, -1)\n",
    "            b_c = BinaryClassifier(self.batch_size, self.max_iter, \n",
    "                                   self.learning_rate, self.random_state, self.C)\n",
    "            b_c.fit(X, y_binary)\n",
    "            self.outputs_.append(b_c)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        prediction = []\n",
    "        for o in self.outputs_:\n",
    "            prediction.append(o.hypothesis(X))\n",
    "        return self.labels[np.argmax(prediction, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4zskV6XyTNX"
   },
   "source": [
    "# MNIST Read Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_cA1d2FxwhE"
   },
   "outputs": [],
   "source": [
    "def read(images, labels):\n",
    "    with open(labels, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def read_no_label(images):\n",
    "    with open(images, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(60000, 784)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NM_gcg_X6u4u"
   },
   "source": [
    "# Read MNIST & Split for Valiation (80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viA-V_NGySjf"
   },
   "outputs": [],
   "source": [
    "                            # 경로 수정하세요 !\n",
    "X, y = read(os.getcwd() + '/drive/My Drive/data/newtrain-images-idx3-ubyte', \n",
    "            os.getcwd() + '/drive/My Drive/data/newtrain-labels-idx1-ubyte')\n",
    "# X_test_no_label = read_no_label(os.getcwd()+'/data/testall-images-idx3-ubyte')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "# print(X_train[0])\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYKNh4VglWOt"
   },
   "source": [
    "# Preprocessing : Convolutional Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBYQuPhc143D"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X_train_scaled = X_train_scaled.reshape(-1,28,28)\n",
    "# X_test_scaled = X_test_scaled.reshape(-1,28,28)\n",
    "# print(X_train_scaled[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8JkQqjYHg4V"
   },
   "outputs": [],
   "source": [
    "# print(X_train_scaled[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGTEGyFTmVuk"
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fX4LRWR3lb3i"
   },
   "outputs": [],
   "source": [
    "def zero_padding(img, n=1):\n",
    "\n",
    "    m = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[2]\n",
    "    \n",
    "    padded_img = np.ones((m, w + 2 * n, h + 2 * n))\n",
    "    \n",
    "    padded_img[:, n : padded_img.shape[1] - n, n : padded_img.shape[2] - n] = img\n",
    "    \n",
    "    return padded_img\n",
    "    \n",
    "def horizontal_filter(img):\n",
    "    h_filter = np.array([\n",
    "        [ 0,  0, 0],\n",
    "        [ 0,  1,  0],\n",
    "        [ -1,  0,  0]\n",
    "    ])\n",
    "    h_filter.reshape(1,3,3)\n",
    "    m = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[2]\n",
    "    horizontal_grad = np.zeros((m, w - 2, h - 2))\n",
    "\n",
    "    for i in range(1, w - 1):\n",
    "        for j in range(1, h - 1):\n",
    "            images_slice = img[:, i - 1 : i + 2, j - 1 : j + 2]\n",
    "            horizontal_grad[:, i - 1, j - 1] = np.sum(np.multiply(images_slice, h_filter), axis=(1, 2))\n",
    "            \n",
    "    return horizontal_grad\n",
    "\n",
    "def horizontal_filter2(img):\n",
    "    h_filter = np.array([\n",
    "        [ -1,  -1,  -1],\n",
    "        [ 0,    1,   0],\n",
    "        [ 1,    1,   1]\n",
    "    ])\n",
    "    h_filter.reshape(1,3,3)\n",
    "    m = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[2]\n",
    "    horizontal_grad = np.zeros((m, w - 2, h - 2))\n",
    "\n",
    "    for i in range(1, w - 1):\n",
    "        for j in range(1, h - 1):\n",
    "            images_slice = img[:, i - 1 : i + 2, j - 1 : j + 2]\n",
    "            horizontal_grad[:, i - 1, j - 1] = np.sum(np.multiply(images_slice, h_filter), axis=(1, 2))\n",
    "            \n",
    "    return horizontal_grad\n",
    "\n",
    "# def horizontal_filter3(img):\n",
    "#     h_filter = np.array([\n",
    "        # [ -1,  -1,  -1],\n",
    "        # [ 0,    1,   0],\n",
    "        # [ 1,    1,   1]\n",
    "#     ])\n",
    "#     h_filter.reshape(1,3,3)\n",
    "#     m = img.shape[0]\n",
    "#     w = img.shape[1]\n",
    "#     h = img.shape[2]\n",
    "#     horizontal_grad = np.zeros((m, w - 2, h - 2))\n",
    "\n",
    "#     for i in range(1, w - 1):\n",
    "#         for j in range(1, h - 1):\n",
    "#             images_slice = img[:, i - 1 : i + 2, j - 1 : j + 2]\n",
    "#             horizontal_grad[:, i - 1, j - 1] = np.sum(np.multiply(images_slice, h_filter), axis=(1, 2))\n",
    "            \n",
    "#     return horizontal_grad\n",
    "    \n",
    "def MaxPooling(img):\n",
    "\n",
    "    m = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[2]\n",
    "    \n",
    "    img = zero_padding(img, 1)\n",
    "    img2 = horizontal_filter(img)\n",
    "    img3 = horizontal_filter2(img)\n",
    "    # img4 = horizontal_filter3(img)\n",
    "    \n",
    "    pooling_grad = np.zeros((m, w//2, h//2))\n",
    "    pooling_grad2 = np.zeros((m, w//2, h//2))\n",
    "    # pooling_grad3 = np.zeros((m, w//2, h//2))\n",
    "    # print('pooling_grade.shape[0]', pooling_grad.shape[0])\n",
    "\n",
    "    for i in range(0, w//2):\n",
    "        for j in range(0, h//2):\n",
    "            pooling_grad[: , i , j ] = np.max(img2[: , 2*i : 2*i + 2, 2*j : 2*j + 2])\n",
    "            pooling_grad2[: , i , j ] = np.max(img3[: , 2*i : 2*i + 2, 2*j : 2*j + 2])\n",
    "            # pooling_grad3[: , i , j ] = np.max(img4[: , 2*i : 2*i + 2, 2*j : 2*j + 2])\n",
    "\n",
    "    pooling_grad = pooling_grad.reshape(m,-1)\n",
    "    pooling_grad2 = pooling_grad.reshape(m,-1)\n",
    "    # pooling_grad3 = pooling_grad.reshape(m,-1)\n",
    "    \n",
    "            \n",
    "    return pooling_grad, pooling_grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5VJlZQkmUUD"
   },
   "source": [
    "## Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVHgeOni7HQn"
   },
   "source": [
    "# Preprocessing : StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33684,
     "status": "ok",
     "timestamp": 1576319728541,
     "user": {
      "displayName": "용일조",
      "photoUrl": "",
      "userId": "09844505769840278881"
     },
     "user_tz": -540
    },
    "id": "kvxkp3reIFCH",
    "outputId": "e7eb02b3-9a29-4d70-afb2-ec87ad14014b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "# X_train_poly_conv = X_train_poly.reshape(X_train_poly.shape[0],-1)\n",
    "# X_test_poly_conv = X_test_poly.reshape(X_test_poly.shape[0], -1)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HB6tbaNm7T5A"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28)\n",
    "\n",
    "X_train_conv, X_train_conv2 = MaxPooling(X_train)\n",
    "X_test_conv, X_test_conv2 = MaxPooling(X_test)\n",
    "\n",
    "# X_train_conv = MaxPooling(X_train)\n",
    "# X_test_conv = MaxPooling(X_test)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "\n",
    "X_train_scaled_conv = np.hstack([X_train,X_train_conv])\n",
    "# X_train_scaled_conv = np.hstack([X_train_conv,X_train_conv2])\n",
    "# X_train_scaled_conv = np.hstack([X_train_conv,X_train_conv3])\n",
    "X_test_scaled_conv = np.hstack([X_test, X_test_conv])\n",
    "# X_test_scaled_conv = np.hstack([X_test_conv, X_test_conv2])\n",
    "# X_test_scaled_conv = np.hstack([X_test_conv, X_test_conv3])\n",
    "\n",
    "# print(X_train_scaled_conv.shape)\n",
    "\n",
    "# X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# X_train_poly = X_train_poly.reshape(X_train.shape[0],-1)\n",
    "# X_test_poly = X_test_poly.reshape(X_test.shape[0],-1)\n",
    "\n",
    "# X_train_conv = X_train_conv.reshape(X_train.shape[0], -1)\n",
    "# X_test_conv = X_test_conv.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SakUhQM57WH7"
   },
   "source": [
    "# Preprocessing : PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54387,
     "status": "ok",
     "timestamp": 1576319749257,
     "user": {
      "displayName": "용일조",
      "photoUrl": "",
      "userId": "09844505769840278881"
     },
     "user_tz": -540
    },
    "id": "VNXZi3G1Hap8",
    "outputId": "c1381141-fa24-4225-f28c-aad72ef9ca78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(56000, 980)\n"
     ]
    }
   ],
   "source": [
    "# X_train_scaled = X_train_scaled.reshape(-1, 28*28)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_scaled_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMiaLfC0Sv2R"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "X_train_pca = pca.fit_transform(X_train) \n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MV4HbBAZ7eUr"
   },
   "source": [
    "# Preprocessing : Polynomial Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aogZ4O0I0S12"
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "X_train_poly = poly.fit_transform(X_train_pca)\n",
    "# X_train_poly2 = poly.fit_transform(X_train_conv2)\n",
    "# print('train is done')\n",
    "X_test_poly = poly.transform(X_test_pca)\n",
    "# # X_test_poly2 = poly.transform(X_test_conv2)\n",
    "\n",
    "# print('train is done2')\n",
    "\n",
    "X_train_conv_scaled_pca_poly = np.hstack([X_train_scaled_conv,X_train_poly])\n",
    "# # X_train_conv_scaled_poly = np.hstack([X_train_conv_scaled_poly,X_train_poly2])\n",
    "X_test_conv_scaled_pca_poly = np.hstack([X_test_scaled_conv,X_test_poly])\n",
    "# # X_test_conv_scaled_poly = np.hstack([X_test_conv_scaled_poly,X_test_poly2])\n",
    "\n",
    "print(X_train_conv_scaled_pca_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6WaCb0g7uV_"
   },
   "source": [
    "## Check how many features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mevK6cEc8EoL"
   },
   "source": [
    "# Set the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llpouwU62bvk"
   },
   "outputs": [],
   "source": [
    "MC=MulticlassClassifier(C=1000, learning_rate=0.01, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yXxo4nl8N5C"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_xSCTXcp8jV5",
    "outputId": "91995ec5-5844-40b6-cf70-452027673127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time :  2019-12-14 10:35:55.435081\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Time : \", datetime.now())\n",
    "\n",
    "start = time.time()\n",
    "MC.fit(X_train_conv_scaled_pca_poly, y_train)\n",
    "\n",
    "print(\"End Time : \", datetime.now())\n",
    "print(\"Training Time : \", time.time() - start)\n",
    "y_pred = MC.predict(X_test_conv_scaled_pca_poly)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeKh4PZF8kl8"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bR90WGMvd8l"
   },
   "outputs": [],
   "source": [
    "y_pred = MC.predict(X_test_conv_scaled_pca_poly)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7x0l951tm5M"
   },
   "source": [
    "1200초 0.97358\n",
    "0 0 0\n",
    "0 1 0\n",
    "-1 0 0\n",
    "\n",
    "-1 -2 -1\n",
    "0 0 0\n",
    "1 2 1\n",
    "\n",
    "\n",
    "1480초 0.973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70Z27LpNegNp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "if (5 == 5): print (\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TEAM4_Poly_Parallel_Conv.ipynb의 사본",
   "provenance": [
    {
     "file_id": "1qhbdRUMW_AQWQRq8d3OcTjpS791UPemi",
     "timestamp": 1575314317465
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
